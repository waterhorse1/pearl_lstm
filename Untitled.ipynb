{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da47a99-0e86-46e8-8a34-3dc13e281d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4014ae3-09a7-41fc-b39a-6cdd350ae6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7543e84b-4620-41e5-b849-ce07079f0ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f27876cf-2edb-49ab-8bdc-909d635cdacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9c4b925-3133-4c2e-a211-46de812ffd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_train_test_task_idx(train_task_num_list, task_num_list):\n",
    "    # train 2, test 2 for HalfCheetahDirEnv, train 25, test 5 for HalfCheetahVelEnv, train 25, test 5 for HalfCheetahMassEnv\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    idx = 0\n",
    "    for train_task_num, all_task_num in zip(train_task_num_list, task_num_list):\n",
    "        if train_task_num == all_task_num: # direction\n",
    "            train_idx.extend(list(range(idx, idx+train_task_num)))\n",
    "            test_idx.extend(list(range(idx, idx+train_task_num)))\n",
    "        else:\n",
    "            train_idx.extend(list(range(idx, idx+train_task_num)))\n",
    "            test_idx.extend(list(range(idx+train_task_num, idx+all_task_num)))\n",
    "        idx += all_task_num\n",
    "    return train_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bcbf600-e4fc-4850-a7ee-14598c7a1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task_num_list = [2, 25, 25]\n",
    "task_num_list = [2, 30, 30]\n",
    "train_idx, test_idx = sample_train_test_task_idx(train_task_num_list, task_num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d9e9e33-8419-440f-85ce-e281712d155b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 27, 28, 29, 30, 31, 57, 58, 59, 60, 61]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ebad4f6-7f2e-4e35-90ea-6dc688975d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import init\n",
    "\n",
    "\n",
    "class ZeroNet(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.zeros(1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "            self, output_shape,\n",
    "            base_type,\n",
    "            append_hidden_shapes=[],\n",
    "            append_hidden_init_func=init.basic_init,\n",
    "            net_last_init_func=init.uniform_init,\n",
    "            activation_func=F.relu,\n",
    "            **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = base_type(activation_func=activation_func, **kwargs)\n",
    "        self.activation_func = activation_func\n",
    "        append_input_shape = self.base.output_shape\n",
    "        self.append_fcs = []\n",
    "        for i, next_shape in enumerate(append_hidden_shapes):\n",
    "            fc = nn.Linear(append_input_shape, next_shape)\n",
    "            append_hidden_init_func(fc)\n",
    "            self.append_fcs.append(fc)\n",
    "            # set attr for pytorch to track parameters( device )\n",
    "            self.__setattr__(\"append_fc{}\".format(i), fc)\n",
    "            append_input_shape = next_shape\n",
    "\n",
    "        self.last = nn.Linear(append_input_shape, output_shape)\n",
    "        net_last_init_func(self.last)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "\n",
    "        for append_fc in self.append_fcs:\n",
    "            out = append_fc(out)\n",
    "            out = self.activation_func(out)\n",
    "\n",
    "        out = self.last(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FlattenNet(Net):\n",
    "    def forward(self, input):\n",
    "        out = torch.cat(input, dim = -1)\n",
    "        return super().forward(out)\n",
    "\n",
    "\n",
    "def null_activation(x):\n",
    "    return x\n",
    "\n",
    "class MLPBase(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_shapes, activation_func=F.relu, init_func = init.basic_init, last_activation_func = None ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.activation_func = activation_func\n",
    "        self.fcs = []\n",
    "        if last_activation_func is not None:\n",
    "            self.last_activation_func = last_activation_func\n",
    "        else:\n",
    "            self.last_activation_func = activation_func\n",
    "        input_shape = np.prod(input_shape)\n",
    "\n",
    "        self.output_shape = input_shape\n",
    "        for i, next_shape in enumerate( hidden_shapes ):\n",
    "            fc = nn.Linear(input_shape, next_shape)\n",
    "            init_func(fc)\n",
    "            self.fcs.append(fc)\n",
    "            # set attr for pytorch to track parameters( device )\n",
    "            self.__setattr__(\"fc{}\".format(i), fc)\n",
    "\n",
    "            input_shape = next_shape\n",
    "            self.output_shape = next_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = x\n",
    "        for fc in self.fcs[:-1]:\n",
    "            out = fc(out)\n",
    "            out = self.activation_func(out)\n",
    "        out = self.fcs[-1](out)\n",
    "        out = self.last_activation_func(out)\n",
    "        return out\n",
    "    \n",
    "class ModularGatedCascadeCondNet(nn.Module):\n",
    "    def __init__(self, output_shape,\n",
    "            base_type, em_input_shape, input_shape,\n",
    "            em_hidden_shapes,\n",
    "            hidden_shapes,\n",
    "\n",
    "            num_layers, num_modules,\n",
    "\n",
    "            module_hidden,\n",
    "\n",
    "            gating_hidden, num_gating_layers,\n",
    "\n",
    "            # gated_hidden\n",
    "            add_bn = True,\n",
    "            pre_softmax = False,\n",
    "            cond_ob = True,\n",
    "            module_hidden_init_func = init.basic_init,\n",
    "            last_init_func = init.uniform_init,\n",
    "            activation_func = F.relu,\n",
    "             **kwargs ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = base_type( \n",
    "                        last_activation_func = null_activation,\n",
    "                        input_shape = input_shape,\n",
    "                        activation_func = activation_func,\n",
    "                        hidden_shapes = hidden_shapes,\n",
    "                        **kwargs )\n",
    "        self.em_base = base_type(\n",
    "                        last_activation_func = null_activation,\n",
    "                        input_shape = em_input_shape,\n",
    "                        activation_func = activation_func,\n",
    "                        hidden_shapes = em_hidden_shapes,\n",
    "                        **kwargs )\n",
    "\n",
    "        self.activation_func = activation_func\n",
    "\n",
    "        module_input_shape = self.base.output_shape\n",
    "        self.layer_modules = []\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.num_modules = num_modules\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            layer_module = []\n",
    "            for j in range( num_modules ):\n",
    "                fc = nn.Linear(module_input_shape, module_hidden)\n",
    "                module_hidden_init_func(fc)\n",
    "                if add_bn:\n",
    "                    module = nn.Sequential(\n",
    "                        nn.BatchNorm1d(module_input_shape),\n",
    "                        fc,\n",
    "                        nn.BatchNorm1d(module_hidden)\n",
    "                    )\n",
    "                else:\n",
    "                    module = fc\n",
    "\n",
    "                layer_module.append(module)\n",
    "                self.__setattr__(\"module_{}_{}\".format(i,j), module)\n",
    "\n",
    "            module_input_shape = module_hidden\n",
    "            self.layer_modules.append(layer_module)\n",
    "\n",
    "        self.last = nn.Linear(module_input_shape, output_shape)\n",
    "        last_init_func( self.last )\n",
    "\n",
    "        assert self.em_base.output_shape == self.base.output_shape, \\\n",
    "            \"embedding should has the same dimension with base output for gated\" \n",
    "        gating_input_shape = self.em_base.output_shape\n",
    "        self.gating_fcs = []\n",
    "        for i in range(num_gating_layers):\n",
    "            gating_fc = nn.Linear(gating_input_shape, gating_hidden)\n",
    "            module_hidden_init_func(gating_fc)\n",
    "            self.gating_fcs.append(gating_fc)\n",
    "            self.__setattr__(\"gating_fc_{}\".format(i), gating_fc)\n",
    "            gating_input_shape = gating_hidden\n",
    "\n",
    "        self.gating_weight_fcs = []\n",
    "        self.gating_weight_cond_fcs = []\n",
    "\n",
    "        self.gating_weight_fc_0 = nn.Linear(gating_input_shape,\n",
    "                    num_modules * num_modules )\n",
    "        last_init_func( self.gating_weight_fc_0)\n",
    "        # self.gating_weight_fcs.append(self.gating_weight_fc_0)\n",
    "\n",
    "        for layer_idx in range(num_layers-2):\n",
    "            gating_weight_cond_fc = nn.Linear((layer_idx+1) * \\\n",
    "                                               num_modules * num_modules,\n",
    "                                              gating_input_shape)\n",
    "            module_hidden_init_func(gating_weight_cond_fc)\n",
    "            self.__setattr__(\"gating_weight_cond_fc_{}\".format(layer_idx+1),\n",
    "                             gating_weight_cond_fc)\n",
    "            self.gating_weight_cond_fcs.append(gating_weight_cond_fc)\n",
    "\n",
    "            gating_weight_fc = nn.Linear(gating_input_shape,\n",
    "                                         num_modules * num_modules)\n",
    "            last_init_func(gating_weight_fc)\n",
    "            self.__setattr__(\"gating_weight_fc_{}\".format(layer_idx+1),\n",
    "                             gating_weight_fc)\n",
    "            self.gating_weight_fcs.append(gating_weight_fc)\n",
    "\n",
    "        self.gating_weight_cond_last = nn.Linear((num_layers-1) * \\\n",
    "                                                 num_modules * num_modules,\n",
    "                                                 gating_input_shape)\n",
    "        module_hidden_init_func(self.gating_weight_cond_last)\n",
    "\n",
    "        self.gating_weight_last = nn.Linear(gating_input_shape, num_modules)\n",
    "        last_init_func( self.gating_weight_last )\n",
    "\n",
    "        self.pre_softmax = pre_softmax\n",
    "        self.cond_ob = cond_ob\n",
    "\n",
    "    def forward(self, x, embedding_input, return_weights = False):\n",
    "        # Return weights for visualization\n",
    "        out = self.base(x)\n",
    "        embedding = self.em_base(embedding_input)\n",
    "\n",
    "        if self.cond_ob:\n",
    "            embedding = embedding * out\n",
    "\n",
    "        out = self.activation_func(out)\n",
    "\n",
    "        if len(self.gating_fcs) > 0:\n",
    "            embedding = self.activation_func(embedding)\n",
    "            for fc in self.gating_fcs[:-1]:\n",
    "                embedding = fc(embedding)\n",
    "                embedding = self.activation_func(embedding)\n",
    "            embedding = self.gating_fcs[-1](embedding)\n",
    "\n",
    "        base_shape = embedding.shape[:-1]\n",
    "\n",
    "        weights = []\n",
    "        flatten_weights = []\n",
    "\n",
    "        raw_weight = self.gating_weight_fc_0(self.activation_func(embedding))\n",
    "\n",
    "        weight_shape = base_shape + torch.Size([self.num_modules,\n",
    "                                                self.num_modules])\n",
    "        flatten_shape = base_shape + torch.Size([self.num_modules * \\\n",
    "                                                self.num_modules])\n",
    "\n",
    "        raw_weight = raw_weight.view(weight_shape)\n",
    "\n",
    "        softmax_weight = F.softmax(raw_weight, dim=-1)\n",
    "        weights.append(softmax_weight)\n",
    "        if self.pre_softmax:\n",
    "            flatten_weights.append(raw_weight.view(flatten_shape))\n",
    "        else:\n",
    "            flatten_weights.append(softmax_weight.view(flatten_shape))\n",
    "\n",
    "        for gating_weight_fc, gating_weight_cond_fc in zip(self.gating_weight_fcs, self.gating_weight_cond_fcs):\n",
    "            cond = torch.cat(flatten_weights, dim=-1)\n",
    "            if self.pre_softmax:\n",
    "                cond = self.activation_func(cond)\n",
    "            cond = gating_weight_cond_fc(cond)\n",
    "            cond = cond * embedding\n",
    "            cond = self.activation_func(cond)\n",
    "\n",
    "            raw_weight = gating_weight_fc(cond)\n",
    "            raw_weight = raw_weight.view(weight_shape)\n",
    "            softmax_weight = F.softmax(raw_weight, dim=-1)\n",
    "            weights.append(softmax_weight)\n",
    "            if self.pre_softmax:\n",
    "                flatten_weights.append(raw_weight.view(flatten_shape))\n",
    "            else:\n",
    "                flatten_weights.append(softmax_weight.view(flatten_shape))\n",
    "\n",
    "        cond = torch.cat(flatten_weights, dim=-1)\n",
    "        if self.pre_softmax:\n",
    "            cond = self.activation_func(cond)\n",
    "        cond = self.gating_weight_cond_last(cond)\n",
    "        cond = cond * embedding\n",
    "        cond = self.activation_func(cond)\n",
    "\n",
    "        raw_last_weight = self.gating_weight_last(cond)\n",
    "        last_weight = F.softmax(raw_last_weight, dim = -1)\n",
    "\n",
    "        module_outputs = [(layer_module(out)).unsqueeze(-2) \\\n",
    "                for layer_module in self.layer_modules[0]]\n",
    "\n",
    "        module_outputs = torch.cat(module_outputs, dim = -2 )\n",
    "\n",
    "        # [TODO] Optimize using 1 * 1 convolution.\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            new_module_outputs = []\n",
    "            for j, layer_module in enumerate(self.layer_modules[i + 1]):\n",
    "                module_input = (module_outputs * \\\n",
    "                    weights[i][..., j, :].unsqueeze(-1)).sum(dim=-2)\n",
    "\n",
    "                module_input = self.activation_func(module_input)\n",
    "                new_module_outputs.append((\n",
    "                        layer_module(module_input)\n",
    "                ).unsqueeze(-2))\n",
    "\n",
    "            module_outputs = torch.cat(new_module_outputs, dim = -2)\n",
    "\n",
    "        out = (module_outputs * last_weight.unsqueeze(-1)).sum(-2)\n",
    "        out = self.activation_func(out)\n",
    "        out = self.last(out)\n",
    "\n",
    "        if return_weights:\n",
    "            return out, weights, last_weight\n",
    "        return out\n",
    "\n",
    "\n",
    "class FlattenModularGatedCascadeCondNet(ModularGatedCascadeCondNet):\n",
    "    def forward(self, input, embedding_input, return_weights = False):\n",
    "        out = torch.cat( input, dim = -1 )\n",
    "        return super().forward(out, embedding_input, return_weights = return_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8068ed28-a67a-45a8-b49a-8985b43f1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModularGatedCascadeCondNet(output_shape=20, base_type=MLPBase, em_input_shape=32, input_shape=400, em_hidden_shapes=em_hidden_shapes,\n",
    "                          hidden_shapes=hidden_shapes, num_layers=num_layers, num_modules=num_modules,\n",
    "                          module_hidden=module_hidden, gating_hidden=gating_hidden, num_gating_layers=num_gating_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6100976-4978-4c22-8918-cbefbb23e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_shapes = [400, 400]\n",
    "em_hidden_shapes = [400]\n",
    "num_layers = 2\n",
    "num_modules = 2\n",
    "module_hidden = 256\n",
    "num_gating_layers = 2\n",
    "gating_hidden = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88c46c14-e7ef-4986-9a96-7ad387056878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 20])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(40,400), torch.randn(40,32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35940c5e-a2c9-4a09-9e9e-558aa585df7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "quantum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
